# Kaibot

![kaibot](docs/images/kaibot.png)

## Overview
Here and there we see numerous predictions that pretty soon chatbots will play a key role in the communication between the users and their systems. We don't have a crystal ball and we don't want to wait for this "pretty soon", so we decided to make these prophecies come true now and see what it looks like. Just for the sake of curiosity. 

We implemented a bot named Kaibot (He/His/Him). He can keep a simple conversation with you and start a GitHub Actions workflow to perform end-to-end testing.

Your laptop serves as a communication device, as ears and mouth. Potentially, the ears and mouth will be moved to Raspberry PI and implanted into a physical robot body (coming soon). The body is a backend system for which we are building the conversational UI. In case of Kaibot this is currently GitHub. And the brain which is capable of chatting and controlling the body is a simple chatbot in the cloud implemented with a Luis application recognizing the intent and an Azure Function that handles the conversation flow. In the next version, as Kaibot grows up, the Azure Function will be substituted with Azure Bot Service to handle more sophisticated conversations.

The key concept in the Kaibot implementation is that the brain, the chatbot, is decoupled from the details of the communication devices and from the details of the GitHub connections and workflow. All connections are implemented with an intermediate API layer, a glue, something between the brain and the outer world. It’s kind of a neuro system. This system is implemented with Azure Functions. For example, when an ear sends a message to the brain it does it through a neuro system (Azure Function). This function knows how to connect to the chatbot, what endpoints and tokens to use, how to construct an http request that the chatbot's webhook understands and so on. With this approach we can easily use the same ear with another chatbot powered by Google, Amazon, IBM, or we can use this chatbot with different ears implementation, it’s not necessarily ears, it can be just a text messenger.  


## Usage

### Prerequisites 
- Luis application with [src/luis/kaibot.lu](src/luis/kaibot.lu) definition
- KaibotBrain Azure Function application deployed from [src/brain]
- KaibotGlue Azure Function application deployed from [src/glue]

### Playing
- Open external IP tunnel to your laptop
    - ngrok start --config tunnels.yml --all
    - configure in KaibotGlue an env variable with the URI generated by ngrok
      YOURNAME_MOUTH_URL (e.g. EUGENE_MOUTH_URL)
- [start mouth](src/mouth/readme.md)
- [start ears](src/ears/readme.md)
